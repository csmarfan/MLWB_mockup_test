{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petrophysical Modeling with Amazon SageMaker XGBoost algorithm\n",
    "_**Single machine training for regression with Amazon SageMaker XGBoost algorithm**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "  1. [Fetching the dataset](#Fetching-the-dataset)\n",
    "  2. [Data Ingestion](#Data-ingestion)\n",
    "3. [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "  1. [Import model into hosting](#Import-model-into-hosting)\n",
    "  2. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "  3. [Create endpoint](#Create-endpoint)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker's implementation of the XGBoost algorithm to train and host a regression model for prediction of porosity from other well log measurements. \n",
    "\n",
    "> A Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. - http://jupyter.org/\n",
    "\n",
    "The dataset is log data from nine wells, with 5 wireline log measurements, two indicator variables and a facies label at half foot intervals. The seven predictor variables are:\n",
    "\n",
    "Five wireline log curves including: \n",
    "\n",
    "- Gamma ray (GR)\n",
    "- Resistivity logging (ILD_log10)\n",
    "- Photoelectric effect (PE) - Note, some wells do not have PE.\n",
    "- Neutron-density porosity difference \n",
    "- Average neutron-density porosity (DeltaPHI and PHIND)\n",
    "\n",
    "Two geologic constraining variables: \n",
    "\n",
    "- Nonmarine-marine indicator (NM_M) \n",
    "- Relative position (RELPOS)\n",
    "\n",
    "We will focus on these logs: GR, ILD_log10, DeltaPH, PHIND, where we try to predict PHIND by training using 7 wells, 1 validation well, and one well for blind test. \n",
    "\n",
    "The dataset comes from a class exercise from The University of Kansas on Neural Networks and Fuzzy Systems. This exercise is based on a consortium project to use machine learning techniques to create a reservoir model of the largest gas fields in North America, the Hugoton and Panoma Fields. For more info on the origin of the data, see Bohling and Dubois (2003) and Dubois et al. (2007).\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "\n",
    "This notebook was created and tested on an ml.m4.4xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "1. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "1. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# bucket='mef-test' # put your s3 bucket name here, and create s3 bucket\n",
    "# # prefix = 'mehdi-test/high-level'\n",
    "# prefix = 'public-data-example/high-level'\n",
    "\n",
    "\n",
    "bucket='sdu-sagemaker-workshop' # put your s3 bucket name here, and create s3 bucket\n",
    "prefix = 'public-data-example/high-level'\n",
    "\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the dataset\n",
    "\n",
    "Following methods split the data into train/test/validation datasets and upload files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import io\n",
    "import boto3\n",
    "import random\n",
    "\n",
    "def data_split(FILE_DATA, FILE_TRAIN, FILE_VALIDATION, FILE_TEST, Valid_Well_Name, Blind_Well_Name, TARGET_VAR):\n",
    "    data = pd.read_csv(FILE_DATA)\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    # Make the first column the target feature    \n",
    "    cols = data.columns.tolist()\n",
    "    target_pos = data.columns.get_loc(TARGET_VAR)\n",
    "    cols.pop(target_pos)\n",
    "    cols = [TARGET_VAR] + cols\n",
    "    data = data.loc[:,cols]\n",
    "                \n",
    "    # Remove target colun from test set\n",
    "    #test_data = test_data.drop([TARGET_VAR], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    test_data = data[data['Well Name'] == Blind_Well_Name]\n",
    "    test_data = test_data.drop(['Well Name'], axis=1)\n",
    "    \n",
    "    valid_data = data[data['Well Name'] == Valid_Well_Name]\n",
    "    valid_data = valid_data.drop(['Well Name'], axis=1)\n",
    "\n",
    "    train_data = data[data['Well Name'] != Blind_Well_Name]\n",
    "    train_data = train_data[train_data['Well Name'] != Valid_Well_Name]\n",
    "\n",
    "    train_data = train_data.drop(['Well Name'], axis=1)\n",
    "\n",
    "    train_data.to_csv(FILE_TRAIN, index=False, header=False)\n",
    "    valid_data.to_csv(FILE_VALIDATION, index=False, header=False)\n",
    "    test_data.to_csv(FILE_TEST, index=False, header=False)\n",
    "    \n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "\n",
    "# def upload_to_s3(bucket, channel, filename):\n",
    "#     fobj=open(filename, 'rb')\n",
    "#     key = prefix+'/'+channel+'/'+filename\n",
    "#     url = 's3://{}/{}'.format(bucket, key)\n",
    "#     print('Writing to {}'.format(url))\n",
    "#     write_to_s3(fobj, bucket, key)\n",
    "#     return(url)\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj=open(filename, 'rb')\n",
    "    key = prefix+'/'+channel+'/'+filename\n",
    "    url = 's3://{}/{}'.format(bucket, key)\n",
    "    print('Writing to {}'.format(url))\n",
    "    write_to_s3(fobj, bucket, key)     \n",
    "    return(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the csv option, here is the critical piece of information about the data format:\n",
    "\n",
    "> For CSV training, the algorithm assumes that the target variable is in the first column and that the CSV does not have a header record. For CSV inference, the algorithm assumes that CSV input does not have the label column.\n",
    "\n",
    "[Source](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric columns\n",
    "import pandas as pd\n",
    "data = pd.read_csv('facies_vectors.csv')\n",
    "# Remove rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data['Well Name'].unique()\n",
    "data = data.loc[:,['Well Name', 'GR', 'ILD_log10', 'DeltaPHI', 'PHIND']]\n",
    "\n",
    "\n",
    "# Write file back to disk\n",
    "data.to_csv('facies_num.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DATA = 'facies_num.csv'\n",
    "TARGET_VAR = 'PHIND'\n",
    "FILE_TRAIN = 'facies_train.csv'\n",
    "FILE_VALIDATION = 'facies_validation.csv'\n",
    "FILE_TEST = 'facies_test.csv'\n",
    "Valid_Well_Name = 'SHRIMPLIN'\n",
    "Blind_Well_Name = 'SHANKLE'\n",
    "\n",
    "data_split(FILE_DATA, FILE_TRAIN, FILE_VALIDATION, FILE_TEST, Valid_Well_Name, Blind_Well_Name, TARGET_VAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the files to the S3 bucket\n",
    "s3_train_loc = upload_to_s3(bucket = bucket, channel = 'train', filename = FILE_TRAIN)\n",
    "s3_valid_loc = upload_to_s3(bucket = bucket, channel = 'validation', filename = FILE_VALIDATION)\n",
    "s3_test_loc = upload_to_s3(bucket = bucket, channel = 'test', filename = FILE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is completed, which in this example, takes between 5 and 6 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost hyperparameter [docs](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Instantiate the estimator\n",
    "xgboost = sagemaker.estimator.Estimator('811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                       sagemaker_session=sagemaker.Session())\n",
    "\n",
    "# Set the hyperparameters\n",
    "xgboost.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='reg:linear',\n",
    "                        num_round=100)\n",
    "\n",
    "# Set the input data formatting and locatiopns\n",
    "s3_input_train = sagemaker.s3_input(s3_data=s3_train_loc, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=s3_valid_loc, content_type='csv')\n",
    "\n",
    "# Train the model\n",
    "xgboost.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgboost_predictor = xgboost.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "Finally, the customer can now validate the model for use. They can obtain the endpoint from the client library using the result from previous operations, and generate classifications from the trained model using that endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## import numpy as np\n",
    "\n",
    "def predict(data, rows=500):\n",
    "    xgboost_predictor.content_type = 'text/csv'\n",
    "    xgboost_predictor.serializer = csv_serializer\n",
    "    xgboost_predictor.deserializer = None\n",
    "\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgboost_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "test_data = pd.read_csv(FILE_TEST, header=None)\n",
    "labels = test_data.iloc[:,0]\n",
    "predictions = predict(test_data.as_matrix()[:, 1:]) # Note that we are not using the first column, which is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def predict(data, rows=500):\n",
    "def predict(data):\n",
    "    rows=len(data)\n",
    "    xgboost_predictor.content_type = 'text/csv'\n",
    "    xgboost_predictor.serializer = csv_serializer\n",
    "    xgboost_predictor.deserializer = None\n",
    "\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgboost_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "test_data = pd.read_csv(FILE_TEST, header=None)\n",
    "labels = test_data.iloc[:,0]\n",
    "predictions = predict(test_data.as_matrix()[:, 1:]) # Note that we are not using the first column, which is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def log_plot(logs):\n",
    "    #make sure logs are sorted by depth\n",
    "    logs = logs.sort_values(by='Depth')\n",
    "\n",
    "    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n",
    "        \n",
    "    f, ax = plt.subplots(nrows=1, ncols=4, figsize=(8, 12))\n",
    "    ax[0].plot(logs.GR, logs.Depth, '-g')\n",
    "    ax[1].plot(logs.ILD_log10, logs.Depth, '-')\n",
    "    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')\n",
    "    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')\n",
    "    ax[3].plot(logs.PPoro, logs.Depth, '-', color='blue')\n",
    "    ax[3].legend(['Measured','Predicted'])\n",
    "    \n",
    "    for i in range(len(ax)):\n",
    "        ax[i].set_ylim(ztop,zbot)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].grid()\n",
    "        ax[i].locator_params(axis='x', nbins=3)\n",
    "    \n",
    "    ax[0].set_xlabel(\"GR\")\n",
    "    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n",
    "    ax[1].set_xlabel(\"ILD_log10\")\n",
    "    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n",
    "    ax[2].set_xlabel(\"DeltaPHI\")\n",
    "    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n",
    "    ax[3].set_xlabel(\"Porosity\")\n",
    "    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n",
    "\n",
    "    \n",
    "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
    "    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)\n",
    "\n",
    "\n",
    "    \n",
    "data = pd.read_csv('facies_vectors.csv')\n",
    "Blindwell = data[data['Well Name'] == Blind_Well_Name]\n",
    "Blindwell['PPoro'] = predictions\n",
    "display(Blindwell.head())\n",
    "log_plot(Blindwell)\n",
    "print('Correlation coeficient = {0:.5f} \\nMean Squared Error = {1:.5f}'.format(\n",
    "    np.corrcoef(predictions,labels)[0,1]\n",
    "    , mean_squared_error(predictions,labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "If you're ready to be done with this notebook, make sure run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgboost_predictor.endpoint)\n",
    "\n",
    "import sagemaker\n",
    "sagemaker.Session().delete_endpoint(xgboost_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
